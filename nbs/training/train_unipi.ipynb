{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bsc/bsc830651/.conda/envs/factcheck/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-10-25 20:19:17,411 - INFO - Task: monolingual\n",
      "2024-10-25 20:19:17,411 - INFO - Tasks path: data/splits/tasks_no_gs_overlap.json\n",
      "2024-10-25 20:19:17,412 - INFO - Languages: ['eng']\n",
      "2024-10-25 20:19:17,412 - INFO - Model: /home/bsc/bsc830651/.cache/huggingface/hub/models--intfloat--multilingual-e5-large/snapshots/ab10c1a7f42e74530fe7ae5be82e6d4f11a719eb\n",
      "2024-10-25 20:19:17,412 - INFO - Output path: None\n",
      "\n",
      "Languages:   0%|          | 0/1 [00:00<?, ?it/s]2024-10-25 20:19:17,415 - INFO - Lang: eng\n",
      "2024-10-25 20:19:17,415 - INFO - Loading posts...\n",
      "2024-10-25 20:19:19,309 - INFO - Loaded 24431\n",
      "2024-10-25 20:19:19,310 - INFO - Time taken: 1.89s\n",
      "\n",
      "2024-10-25 20:19:19,310 - INFO - Loading fact checks..\n",
      "2024-10-25 20:19:26,154 - INFO - Loaded 85734\n",
      "2024-10-25 20:19:26,155 - INFO - Time taken: 6.85s\n",
      "\n",
      "2024-10-25 20:19:26,156 - INFO - Loading model...\n",
      "Languages: 100%|██████████| 1/1 [00:08<00:00,  8.74s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from src.datasets import TextConcatFactCheck, TextConcatPosts\n",
    "from src.models import EmbeddingModel\n",
    "from src import config\n",
    "from src.utils import log_info\n",
    "\n",
    "task_name = \"monolingual\"\n",
    "langs = [\"eng\"]\n",
    "output_path = None\n",
    "model_name = '/home/bsc/bsc830651/.cache/huggingface/hub/models--intfloat--multilingual-e5-large/snapshots/ab10c1a7f42e74530fe7ae5be82e6d4f11a719eb'\n",
    "tasks_path = config.TASKS_PATH\n",
    "posts_path = config.POSTS_PATH\n",
    "fact_checks_path = config.FACT_CHECKS_PATH\n",
    "\n",
    "\"\"\"\n",
    "Run the task with the given parameters.\n",
    "\"\"\"\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "if output_path is not None:\n",
    "    output_path = os.path.join(output_path, task_name, current_time)\n",
    "\n",
    "langs = [\"eng\"] if task_name == \"crosslingual\" else langs\n",
    "\n",
    "log_info(f\"Task: {task_name}\")\n",
    "log_info(f\"Tasks path: {tasks_path}\")\n",
    "log_info(f\"Languages: {langs}\")\n",
    "log_info(f\"Model: {model_name}\")\n",
    "log_info(f\"Output path: {output_path}\\n\")\n",
    "\n",
    "# Paths from config\n",
    "posts_path = config.POSTS_PATH\n",
    "fact_checks_path = config.FACT_CHECKS_PATH\n",
    "gs_path = config.GS_PATH\n",
    "\n",
    "# tasks_path = \"data/splits/tasks_local_dev.json\"\n",
    "ls_k = [1, 3, 5, 10]\n",
    "\n",
    "d_out = {}\n",
    "df_eval = pd.DataFrame(index=ls_k)\n",
    "df_eval.index.name = \"k\"\n",
    "\n",
    "for lang in tqdm(langs, desc=\"Languages\"):\n",
    "    log_info(f\"Lang: {lang}\")\n",
    "    time_start_lang = time()\n",
    "    \n",
    "    log_info(\"Loading posts...\")\n",
    "    time_start = time()\n",
    "    posts = TextConcatPosts(posts_path, tasks_path, task_name=task_name, gs_path=gs_path, lang=lang)\n",
    "    log_info(f\"Loaded {len(posts)}\")\n",
    "    log_info(f\"Time taken: {time() - time_start:.2f}s\\n\")\n",
    "    \n",
    "    log_info(\"Loading fact checks..\")\n",
    "    time_start = time()\n",
    "    fact_checks = TextConcatFactCheck(fact_checks_path, tasks_path, task_name=task_name, lang=lang)\n",
    "    log_info(f\"Loaded {len(fact_checks)}\")\n",
    "    log_info(f\"Time taken: {time() - time_start:.2f}s\\n\")\n",
    "\n",
    "    df_fc = fact_checks.df\n",
    "    # df_posts_train = posts.df_train\n",
    "    df_posts_dev = posts.df_dev\n",
    "    log_info(\"Loading model...\")\n",
    "    # time_start = time()\n",
    "    # model = EmbeddingModel(model_name, df_fc, batch_size=512)\n",
    "    # log_info(f\"Time taken: {time() - time_start:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "# ner_model_path = \"/gpfs/projects/bsc14/abecerr1/hub/models--FacebookAI--xlm-roberta-large-finetuned-conll03-english/snapshots/18f95e9924f3f452df09cc90945073906ef18f1e/\"\n",
    "# ner_tokenizer = AutoTokenizer.from_pretrained(ner_model_path)\n",
    "# ner_model = AutoModelForTokenClassification.from_pretrained(ner_model_path)\n",
    "# ner_classifier = pipeline(\"ner\", model=ner_model, tokenizer=ner_tokenizer, device=\"cuda\")\n",
    "# ner_out = ner_classifier(\"Alya told Jasmine that Andrew could pay with cash..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4012/4012 [00:51<00:00, 77.80it/s]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "# nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "tqdm.pandas()\n",
    "\n",
    "df_posts_train = posts.df_train\n",
    "# df_posts_train[\"entities\"] = df_posts_train[\"full_text\"].progress_apply(lambda x: nlp(x).ents)\n",
    "df_posts_train[\"lemmas\"] = df_posts_train[\"full_text\"].progress_apply(lambda x: [y.lemma_ for y in nlp(x)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 99.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "post_id\n",
       "2     [\", actually, ,, he, be, a, damn, sight, well,...\n",
       "5     [\", cigarette, smoking, do, not, cause, cancer...\n",
       "13    [\", environmentalist, \", Say, Fracking, be, ev...\n",
       "Name: full_text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_posts_train[\"full_text\"].iloc[:3].progress_apply(lambda x: [y.lemma_ for y in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sight',\n",
       " 'president',\n",
       " 'Miss',\n",
       " 'Ardern',\n",
       " 'Judith',\n",
       " 'Collins',\n",
       " 'Donald',\n",
       " 'Trump',\n",
       " 'thanks[SEP']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[y.lemma_ for y in nlp(df_posts_train[\"full_text\"].iloc[0]) if y.is_stop == False and y.is_punct == False and y.is_space == False and y.pos_ in [\"NOUN\", \"PROPN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Jasmine, Andrew)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"Alya told Jasmine that Andrew could pay with cash..\").ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'entities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.conda/envs/factcheck/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'entities'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_posts_train[\u001b[43mdf_posts_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mentities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)]\n",
      "File \u001b[0;32m~/.conda/envs/factcheck/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.conda/envs/factcheck/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'entities'"
     ]
    }
   ],
   "source": [
    "df_posts_train[df_posts_train[\"entities\"].apply(lambda x: len(x) > 0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factcheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
