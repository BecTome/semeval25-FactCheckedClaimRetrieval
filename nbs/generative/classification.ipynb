{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Fact Checks...\n",
      "Loading Fact Checks (English + Clean)...\n",
      "Loading Posts...\n",
      "Loading Posts (English + Clean)...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()   \n",
    "\n",
    "from src import config\n",
    "from src.datasets import TextConcatFactCheck, TextConcatPosts\n",
    "from src.utils import cleaning_spacy, cleaning_spacy_batch\n",
    "\n",
    "tasks_path = config.TASKS_PATH\n",
    "posts_path = config.POSTS_PATH\n",
    "fact_checks_path = config.FACT_CHECKS_PATH\n",
    "gs_path = config.GS_PATH\n",
    "lang = 'fra'\n",
    "task_name = \"monolingual\"\n",
    "\n",
    "print(\"Loading Fact Checks...\")\n",
    "fc = TextConcatFactCheck(fact_checks_path, tasks_path=tasks_path, task_name=task_name, lang=lang, version=\"english\")\n",
    "print(\"Loading Fact Checks (English + Clean)...\")\n",
    "# fc_eng = TextConcatFactCheck(fact_checks_path, tasks_path=tasks_path, task_name=task_name, lang=lang, version=\"english\", cleaning_function=lambda x: cleaning_spacy_batch(x, nlp))\n",
    "\n",
    "print(\"Loading Posts...\")\n",
    "posts = TextConcatPosts(posts_path, tasks_path=tasks_path, task_name=task_name, lang=lang, gs_path=gs_path, version=\"english\")\n",
    "print(\"Loading Posts (English + Clean)...\")\n",
    "# posts_eng = TextConcatPosts(posts_path, tasks_path=tasks_path, task_name=task_name, lang=lang, gs_path=gs_path, version=\"english\", cleaning_function=lambda x: cleaning_spacy_batch(x, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fc = fc.df\n",
    "df_train_posts = posts.df_train\n",
    "df_dev_posts = posts.df_dev\n",
    "\n",
    "df_train_mini = df_train_posts.iloc[:20]\n",
    "df_fc_mini = df_fc.iloc[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "ds_fc = Dataset.from_pandas(df_fc)\n",
    "ds_train = Dataset.from_pandas(df_train_posts)\n",
    "ds_dev = Dataset.from_pandas(df_dev_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.67s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    # device=\"cuda\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipe.tokenizer.eos_token_id,\n",
    "    pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100/100 [00:04<00:00, 24.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "\n",
    "def get_scores_hf(dataset, post, pipe, terminators):\n",
    "    \"\"\"\n",
    "    Compute similarity scores between a post and fact checks using a Hugging Face dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): Hugging Face dataset containing fact checks with a 'full_text' column.\n",
    "        post (str): The post text to compare against.\n",
    "        pipe (callable): The model pipeline for generating scores.\n",
    "        terminators (list): List of token IDs marking the end of the response.\n",
    "\n",
    "    Returns:\n",
    "        list: Indices of the dataset fact checks sorted by similarity scores.\n",
    "    \"\"\"\n",
    "    def generate_scores(batch):\n",
    "        # Prepare messages for the pipeline\n",
    "        batch_messages = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are an expert saying if one text is a fact check of another text. You can only answer with a number from 0 to 1. (e.g. 0.5)\"},\n",
    "                {\"role\": \"user\", \"content\": f\"From 0 to 1, rate how much the following text:\\n{fc}\\nis a fact check of the following text?:\\n{post}\"}\n",
    "            ]\n",
    "            for fc in batch['full_text']\n",
    "        ]\n",
    "\n",
    "        # Process in batches\n",
    "        outputs = pipe(\n",
    "            batch_messages,\n",
    "            max_new_tokens=2,\n",
    "            temperature=0.01,\n",
    "            eos_token_id=terminators,\n",
    "            pad_token_id=pipe.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "        # Extract scores from the pipeline's output\n",
    "        batch_scores = []\n",
    "        for output in outputs:\n",
    "            try:\n",
    "                score = float(output[0][\"generated_text\"][-1][\"content\"])\n",
    "            except ValueError:\n",
    "                score = 0.0\n",
    "            batch_scores.append(score)\n",
    "        return {\"scores\": batch_scores}\n",
    "\n",
    "    # Apply the function to the dataset\n",
    "    dataset = dataset.map(generate_scores, batched=True, batch_size=16)\n",
    "\n",
    "    # Sort indices by scores\n",
    "    sorted_indices = np.argsort(dataset[\"scores\"])[::-1]\n",
    "    return sorted_indices\n",
    "\n",
    "# Example usage\n",
    "dataset = Dataset.from_pandas(df_fc_mini)\n",
    "post_0 = df_train_mini.iloc[0][\"full_text\"]\n",
    "sorted_indices = get_scores_hf(dataset, post_0, pipe, terminators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([99, 98, 97, 96, 95, 94, 93, 92, 91, 90, 89, 88, 87, 86, 85, 84, 83,\n",
       "       82, 81, 80, 79, 78, 77, 76, 75, 74, 73, 72, 71, 70, 69, 68, 67, 66,\n",
       "       65, 64, 63, 62, 61, 60, 59, 58, 57, 56, 55, 54, 53, 52, 51, 50, 49,\n",
       "       48, 47, 46, 45, 44, 43, 42, 41, 40, 39, 38, 37, 36, 35, 34, 33, 32,\n",
       "       31, 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15,\n",
       "       14, 13, 12, 11, 10,  9,  8,  7,  6,  5,  4,  3,  2,  1,  0])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "factcheck",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
